{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Explainers and Metrics in GraphXAI\n",
    "Here, we'll walk through a quick example where we 1) generate a synthetic dataset that is ready for XAI evaluation, 2) train a GNN model, 3) explain the GNN model with two types of explainers, and 4) evaluate the output explanations from each explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from graphxai.datasets import ShapeGGen\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we load the dataset and run a quick plot of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 243/243 [00:01<00:00, 242.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get dataset:\n",
    "torch.set_default_device(\"cpu\")\n",
    "dataset = ShapeGGen(\n",
    "            base_graph= \"ba\",\n",
    "            verify=False,\n",
    "            max_tries_verification=5,\n",
    "            homophily_coef=1.0,\n",
    "            seed=0,\n",
    "            shape_method=\"house\",\n",
    "            sens_attribution_noise=0.3,\n",
    "            num_hops=3,\n",
    "            model_layers=3,\n",
    "            make_explanations=True,\n",
    "            variant=1,\n",
    "            num_subgraphs=300,\n",
    "            prob_connection=0.003,\n",
    "            subgraph_size=11,\n",
    "            # Features=\n",
    "            class_sep=15.,\n",
    "            n_features=100,\n",
    "            n_clusters_per_class=2,\n",
    "            n_informative=10,\n",
    "            add_sensitive_feature=False,\n",
    "            attribute_sensitive_feature=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a model from scratch on the data:\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "\n",
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(self,input_feat, hidden_channels, classes = 2):\n",
    "        super(MyGNN, self).__init__()\n",
    "        self.mlp_gin1 = torch.nn.Linear(input_feat, hidden_channels)\n",
    "        self.gin1 = GINConv(self.mlp_gin1)\n",
    "        self.mlp_gin2 = torch.nn.Linear(hidden_channels, classes)\n",
    "        self.gin2 = GINConv(self.mlp_gin2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # NOTE: our provided testing function assumes no softmax\n",
    "        #   output from the forward call.\n",
    "        x = self.gin1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.gin2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get our data and train the model. The data includes a train, val, and testing mask that are generated when you call `get_graph` on the dataset. These masks are used internally in our provided train, test, and val functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.6491\n",
      "Test AUROC: 0.9564\n"
     ]
    }
   ],
   "source": [
    "from graphxai.gnn_models.node_classification import train, test\n",
    "\n",
    "data = dataset.get_graph(use_fixed_split=True)\n",
    "\n",
    "model = MyGNN(dataset.n_features, 32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model:\n",
    "for _ in range(300):\n",
    "    loss = train(model, optimizer, criterion, data)\n",
    "\n",
    "# Final testing performance:\n",
    "f1, acc, prec, rec, auprc, auroc = test(model, data, num_classes = 2, get_auc = True)\n",
    "\n",
    "print('Test F1 score: {:.4f}'.format(f1))\n",
    "print('Test AUROC: {:.4f}'.format(auroc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model trained, we can move to explaining the performance. We'll use two different explainers here from two different families of GNN explainers provided in `graphxai`: PGExplainer (perturbation-based, paper: https://arxiv.org/abs/2011.04573) and Integrated Gradients (gradient-based, paper: https://arxiv.org/abs/1703.01365).\n",
    "\n",
    "* Note that we usually want better testing performance, but we keep the graph small here for the example. Larger graphs produce better performance overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from graphxai.explainers import PGExplainer,GNNExplainer, IntegratedGradExplainer, GradExplainer, GuidedBP,RandomExplainer\n",
    "\n",
    "# Embedding layer name is final GNN embedding layer in the model\n",
    "# pgex = PGExplainer(model, emb_layer_name = 'gin2', max_epochs = 10, lr = 0.1)\n",
    "# gnnex = GNNExplainer(model)\n",
    "# gradex = GradExplainer(model, criterion=criterion)\n",
    "# guidex= GuidedBP(model)\n",
    "# randex= RandomExplainer(model)\n",
    "# # Required to first train PGExplainer on the dataset:\n",
    "# # Feed in entire data, the internal model uses the data's train mask\n",
    "# pgex.train_explanation_model(data)\n",
    "\n",
    "# # No training with Integrated Gradients, just run the model:\n",
    "# igex = IntegratedGradExplainer(model, criterion=criterion)\n",
    "\n",
    "# # Sample a random node for visualization. Also returns a ground truth explanation:\n",
    "# node_idx, gt_exp = dataset.choose_node(split = 'test')\n",
    "# print(node_idx)\n",
    "\n",
    "    \n",
    "# # Get explanations from both IG and PGEx:\n",
    "# pgex_exp = pgex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index)\n",
    "# gnnex_exp = gnnex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index)\n",
    "# ig_exp = igex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "# grad_exp =gradex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "# guid_exp=guidex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "# rand_exp=randex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "#fig, ax = plt.subplots(1,1, figsize = (10, 8))\n",
    "\n",
    "# Ground-truth explanations always provided as a list. In ShapeGGen, we use the first\n",
    "#   element since it produces unique explanations. \n",
    "#gt_exp[0].visualize_node(num_hops = 3, graph_data = data, ax = ax[0])\n",
    "#pgex_exp.visualize_node(num_hops = 3, graph_data = data, ax = ax[1])\n",
    "#ig_exp.visualize_node(num_hops = 3, graph_data = data, ax = ax[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pygod.metric import eval_roc_auc\n",
    "# from SuspiciousGCN import Both as SGCN\n",
    "def find_indices(lst, condition):\n",
    "    return [i for i, elem in enumerate(lst) if condition(elem)]\n",
    "\n",
    "index_anomaly=find_indices(dataset.y,lambda e:e==1)\n",
    "print(len(index_anomaly))\n",
    "index_normal=find_indices(dataset.y,lambda e:e==0)\n",
    "index_anomaly_train=index_anomaly[0:round(len(index_anomaly)/10)]\n",
    "\n",
    "index_normal_train=index_normal[0:round(len(index_normal)/10)]\n",
    "sample_s =torch.LongTensor(index_anomaly_train)\n",
    "#print(len(sample_s))\n",
    "sample_n =torch.LongTensor(index_normal)\n",
    "\n",
    "# models = SGCN(Sample_n=sample_n, Sample_s= sample_s,epoch=100,hid_dim=20,dropout=0.5,alpha=0.5,batch_size=0,lr=0.01)\n",
    "# models.fit(data)\n",
    "# Rn,x_n,s_n,txn,tsn = models.decision_function(data)\n",
    "# print(x_n)\n",
    "# modeln = SGCN(Sample_n=sample_s, Sample_s= sample_n,epoch=100,hid_dim=20,dropout=0.5,alpha=0.5,batch_size=0,lr=0.01)\n",
    "# modeln.fit(data)\n",
    "# Rs,x_s,s_s,txs,tss = modeln.decision_function(data)\n",
    "# print(tss)\n",
    "# #txn=(txn-txn.min())/(txn.max()-txn.min())\n",
    "# #tsn=(tsn-tsn.min())/(tsn.max()-tsn.min())\n",
    "# #txs=(txs-txs.min())/(txs.max()-txs.min())\n",
    "# #tss=(tss-tss.min())/(tss.max()-tss.min())\n",
    "# #print(tss)\n",
    "# x_n=(x_n-x_n.min())/(x_n.max()-x_n.min())\n",
    "# s_n=(s_n-x_s.min())/(s_n.max()-s_n.min())\n",
    "# x_s=(x_s-x_s.min())/(x_s.max()-x_s.min())\n",
    "# s_s=(s_s-s_s.min())/(s_s.max()-s_s.min())\n",
    "# tsn=tsn.detach().cpu().numpy()\n",
    "# tss=tss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_n.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering,KMeans,OPTICS,SpectralClustering,DBSCAN\n",
    "# from matplotlib import pyplot\n",
    "# import numpy as np\n",
    "# import umap\n",
    "\n",
    "# array=np.column_stack((x_n.detach().cpu().numpy(),s_n.detach().cpu().numpy()))\n",
    "\n",
    "# #array=np.concatenate([txn.detach().cpu().numpy()],axis=1)\n",
    "# #print(array.shape)\n",
    "\n",
    "# # define the model\n",
    "# reducer = umap.UMAP()\n",
    "# embedding = reducer.fit_transform(array)\n",
    "# #print(embedding)\n",
    "# model =DBSCAN(eps=1.0,min_samples=5)\n",
    "# # fit model and predict clusters\n",
    "# #yhat = model.fit_predict(array)\n",
    "# yhat = model.fit_predict(embedding)\n",
    "# # retrieve unique clusters\n",
    "# clusters = np.unique(yhat)\n",
    "# # create scatter plot for samples from each cluster\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     # get row indexes for samples with this cluster\n",
    "#     row_ix = np.where(yhat == cluster)\n",
    "#     #print(row_ix)\n",
    "#     # create scatter of these samples\n",
    "#     pyplot.scatter(embedding[row_ix, 0], embedding[row_ix, 1],alpha=0.5)\n",
    "# pyplot.show()\n",
    "# pyplot.scatter(embedding[index_anomaly, 0], embedding[index_anomaly, 1],c=\"green\")\n",
    "# pyplot.scatter(embedding[index_normal, 0], embedding[index_normal, 1],c=\"red\",alpha=0.05)\n",
    "# pyplot.show()\n",
    "\n",
    "# scoren=np.zeros(len(Rn))\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     score=0\n",
    "#     # get row indexes for samples with this cluster\n",
    "#     row_ix = np.where(yhat == cluster)\n",
    "#     s=Rn[row_ix].mean()\n",
    "#     for i in row_ix:\n",
    "#         scoren[i]=score-len(i)\n",
    "#     #print(row_ix)\n",
    "#     # create scatter of these samples\n",
    "#     pyplot.scatter(Rn[row_ix], Rs[row_ix])\n",
    "# pyplot.show()\n",
    "# pyplot.scatter(Rn[index_anomaly], Rs[index_anomaly],c=\"green\")\n",
    "# pyplot.scatter(Rn[index_normal], Rs[index_normal],c=\"red\")\n",
    "\n",
    "# pyplot.show()\n",
    "\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), scoren)\n",
    "# print(auc_score)\n",
    "# #print(row_ix)\n",
    "# #print(index_anomaly)\n",
    "# #print(txn.detach().cpu().numpy())\n",
    "# #array=np.column_stack((txn.detach().cpu().numpy(),tsn))\n",
    "\n",
    "\n",
    "# #-----------------------------------------------------------------------------------------------------------------------\n",
    "# array=np.column_stack((x_s.detach().cpu().numpy(),s_s.detach().cpu().numpy()))\n",
    "# #array=np.column_stack((txs.detach().cpu().numpy(),tss))\n",
    "\n",
    "# array=np.concatenate([txs.detach().cpu().numpy()],axis=1)\n",
    "# print(array.shape)\n",
    "\n",
    "# # define the model\n",
    "# reducer = umap.UMAP()\n",
    "# embedding = reducer.fit_transform(array)\n",
    "# #print(embedding)\n",
    "# model =DBSCAN(eps=0.5,min_samples=2)\n",
    "# # fit model and predict clusters\n",
    "# #yhat = model.fit_predict(array)\n",
    "# yhat = model.fit_predict(embedding)\n",
    "# # retrieve unique clusters\n",
    "# clusters = np.unique(yhat)\n",
    "# # create scatter plot for samples from each cluster\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     # get row indexes for samples with this cluster\n",
    "#     row_ix = np.where(yhat == cluster)\n",
    "#     #print(row_ix)\n",
    "#     # create scatter of these samples\n",
    "#     pyplot.scatter(embedding[row_ix, 0], embedding[row_ix, 1],alpha=0.5)\n",
    "# pyplot.show()\n",
    "# pyplot.scatter(embedding[index_anomaly, 0], embedding[index_anomaly, 1],c=\"green\")\n",
    "# pyplot.scatter(embedding[index_normal, 0], embedding[index_normal, 1],c=\"red\",alpha=0.05)\n",
    "# pyplot.show()\n",
    "\n",
    "# scores=np.zeros(len(Rs))\n",
    "# for cluster in clusters:\n",
    "#     # get row indexes for samples with this cluster\n",
    "#     row_ix = np.where(yhat == cluster)\n",
    "#     s=Rs[row_ix].mean()\n",
    "#     for i in row_ix:\n",
    "#         scores[i]=s+len(i)\n",
    "#     #print(row_ix)\n",
    "#     # create scatter of these samples\n",
    "#     pyplot.scatter(Rn[row_ix], Rs[row_ix])\n",
    "# pyplot.show()\n",
    "# pyplot.scatter(Rn[index_anomaly], Rs[index_anomaly],c=\"green\")\n",
    "# pyplot.scatter(Rn[index_normal], Rs[index_normal],c=\"red\")\n",
    "\n",
    "# pyplot.show()\n",
    "\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), -scores)\n",
    "# print(auc_score)\n",
    "\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), scoren/scores)\n",
    "# print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sx=x_n-x_s\n",
    "# #print(x)\n",
    "# ss=s_n/s_s\n",
    "# #prints_s)\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), Rn/Rs)\n",
    "# print(auc_score)\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), Rn)\n",
    "# print(auc_score)\n",
    "# auc_score = eval_roc_auc(data.y.detach().cpu(), -Rs)\n",
    "# print(auc_score)\n",
    "# sum=(sx.sum(1)+ss.sum(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum.shape\n",
    "# sx.shape\n",
    "# ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sx=torch.div(sx.t(),sum).t()\n",
    "# print(sx.shape)\n",
    "# ss=torch.div(ss.t(),sum).t()\n",
    "# print(ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Susps=ss\n",
    "# Suspx=sx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset, sub_edge_index, mapping, hard_edge_mask = k_hop_subgraph(node_idx, 3, data.edge_index, relabel_nodes=True)\n",
    "# print(len(sub_edge_index[0]))\n",
    "\n",
    "# # subSusps=np.empty(0)\n",
    "# # sub_edge_index=sub_edge_index.detach().cpu().numpy()\n",
    "# # st=ss.detach().cpu().numpy()\n",
    "# # for i in range(0,len(sub_edge_index[0])):\n",
    "# #     subSusps=np.append(subSusps,st[sub_edge_index[0][i]][sub_edge_index[1][i]]+st[sub_edge_index[1][i]][sub_edge_index[0][i]])\n",
    "\n",
    "\n",
    "    \n",
    "# #print((subSusps))\n",
    "# print(len(rand_exp.edge_imp))\n",
    "# print(len(gnnex_exp.edge_imp))\n",
    "# print(len(dataset.explanations[node_idx][0].edge_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pygod.detector import DOMINANT,AnomalyDAE\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "from torch_geometric.utils import to_dense_adj,to_networkx,from_networkx\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "modeld = DOMINANT(epoch=200,hid_dim=10,dropout=0.5,alpha=0.5,batch_size=0,gpu=0)\n",
    "modeld.fit(data)\n",
    "#D,x,s=modeld.decision_function(data)\n",
    "pred, D, prob, conf = modeld.predict(data,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "x=modeld.x_imp\n",
    "s=modeld.a_imp\n",
    "auc_score = eval_roc_auc(data.y.detach().cpu(), D)\n",
    "rank=stats.rankdata(D)\n",
    "#conf_ex=ExplainerConfig(\"phenomenon\",\"attributes\",\"object\")\n",
    "# print(auc_score)\n",
    "# print(len(s.sum(1)))\n",
    "# print(len(x.sum(1)))\n",
    "# sum=(s.sum(1)+x.sum(1))\n",
    "\n",
    "# x=torch.div(x.t(),sum).t()\n",
    "# s=torch.div(s.t(),sum).t()\n",
    "# subDoms=np.empty(0)\n",
    "# subset, sub_edge_index, mapping, hard_edge_mask = k_hop_subgraph(node_idx, 3, data.edge_index, relabel_nodes=True)\n",
    "# st=s.detach().cpu().numpy()\n",
    "# for i in range(0,len(sub_edge_index[0])):\n",
    "#     subDoms=np.append(subDoms,st[sub_edge_index[0][i]][sub_edge_index[1][i]]+st[sub_edge_index[1][i]][sub_edge_index[0][i]])\n",
    "\n",
    "def count(gt,exp): \n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(0,len(gt)):\n",
    "        if exp[i]:\n",
    "            if gt[i]==exp[i]:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if gt[i]:\n",
    "                FN+=1\n",
    "                \n",
    "    return([TP / (TP + FP + FN + 1e-09),TP/(TP+FP+1e-09),TP/(FN+TP+1e-09)])\n",
    "def Charax(data,exp,node_idx): \n",
    "    x=data.x.clone()\n",
    "    ind=torch.nonzero(exp, as_tuple=False)[:, 0]\n",
    "    for k in ind:\n",
    "        x[node_idx][k]=0\n",
    "    dataalt=Data(x=x,edge_index=data.edge_index,y=data.y)\n",
    "    predalt, scorealt, probalt, confalt = modeld.predict(dataalt,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "    rankalt=stats.rankdata(scorealt)\n",
    "    faithx=((rank[node_idx]-rankalt[node_idx])/data.x.shape[0])\n",
    "    x=data.x.clone()\n",
    "    tempx=data.x.clone()\n",
    "    x[node_idx]=torch.zeros(x[node_idx].shape[0])\n",
    "    for k in ind:\n",
    "            x[node_idx][k]=tempx[node_idx][k]\n",
    "    torch.cuda.empty_cache()\n",
    "    dataalt=Data(x=x,edge_index=data.edge_index,y=data.y)\n",
    "    predalt, scorealt, probalt, confalt = modeld.predict(dataalt,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "    rankalt=stats.rankdata(scorealt)\n",
    "    completx=((rank[node_idx]-rankalt[node_idx])/data.x.shape[0])\n",
    "\n",
    "    return faithx, completx\n",
    "def Charas(data,edges_exp,sub,node_idx):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    sub_nodes, sub_edge_index, mapping, edge_mask = sub\n",
    "    NecG=G.copy()\n",
    "    torch.cuda.empty_cache()\n",
    "    ind=torch.nonzero(edges_exp, as_tuple=False)[:, 0]\n",
    "    for k in ind:\n",
    "        fir=sub_edge_index[0,k].item()\n",
    "        sec=sub_edge_index[1,k].item()\n",
    "        if(NecG.has_edge(fir,sec)):\n",
    "            NecG.remove_edge(fir,sec)\n",
    "    for temp in range(0,data.x.shape[0]):\n",
    "        NecG.add_edge(temp,temp)\n",
    "    dataalt=Data(x=data.x,edge_index=from_networkx(NecG).edge_index,y=data.y)\n",
    "    predalt, scorealt, probalt, confalt = modeld.predict(dataalt,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "    rankalt=stats.rankdata(scorealt)\n",
    "    faith=((rank[node_idx]-rankalt[node_idx])/data.x.shape[0])\n",
    "    CompG=G.copy()\n",
    "    for sub in range(0,len(sub_edge_index[0])):\n",
    "        fir=sub_edge_index[0,sub].item()\n",
    "        sec=sub_edge_index[1,sub].item()\n",
    "        if(CompG.has_edge(fir,sec)):\n",
    "            CompG.remove_edge(fir,sec)\n",
    "    for temp in range(0,data.x.shape[0]):\n",
    "        CompG.add_edge(temp,temp)\n",
    "    for k in ind:\n",
    "        fir=sub_edge_index[0,k].item()\n",
    "        sec=sub_edge_index[1,k].item()\n",
    "        CompG.add_edge(fir,sec)\n",
    "    dataalt=Data(x=data.x,edge_index=from_networkx(CompG).edge_index,y=data.y)\n",
    "    predalt, scorealt, probalt, confalt = modeld.predict(dataalt,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "    rankalt=stats.rankdata(scorealt)\n",
    "    complet=((rank[node_idx]-rankalt[node_idx])/data.x.shape[0])\n",
    "    return faith,complet\n",
    "\n",
    "def get_topk_mask(tensor, k, dim=0, largest=True):\n",
    "    # Use torch.topk to efficiently find top-k elements and their indices\n",
    "    values, indices = torch.topk(tensor, k)\n",
    "\n",
    "    # Create a boolean mask with the same shape as the input tensor\n",
    "    mask = torch.zeros_like(tensor, dtype=torch.bool)\n",
    "\n",
    "    # Scatter elements along the specified dimension based on indices\n",
    "    mask[indices]=1\n",
    "\n",
    "    return mask\n",
    "    \n",
    "def meancountexpl(gt,ex,index_anomaly,data,kx,ks,edge=True,feature=True,lbl_needed=True,hops=True):\n",
    "    moyfeature=np.empty(3)\n",
    "    moyedge=np.empty(3)\n",
    "    i=0\n",
    "    for node_idx in index_anomaly:\n",
    "        i+=1\n",
    "        if hops:\n",
    "            if lbl_needed:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"),label=1, edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "            else:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"),label=1, edge_index = data.edge_index.to(\"cuda:0\"))\n",
    "        else:\n",
    "            if lbl_needed:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"),label=1, edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "            else:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"),label=1, edge_index = data.edge_index.to(\"cuda:0\"))\n",
    "        if feature:\n",
    "            moyfeature=np.vstack([moyfeature,count(gt[node_idx][0].feature_imp,get_topk_mask(exp.feature_imp,kx))])\n",
    "        if edge:\n",
    "            moyedge=np.vstack([moyedge,count(gt[node_idx][0].edge_imp,get_topk_mask(exp.edge_imp,ks))])\n",
    "    return moyfeature.mean(axis=0),moyedge.mean(axis=0)\n",
    "def meancountexplv2(gt,ex,index_anomaly,data,k,edge=True,feature=True,lbl_needed=True,hops=True,y=True):\n",
    "    moyfeature=np.empty((round(k))*5)\n",
    "    moyedge=np.empty((round(k))*5)\n",
    "    f=0\n",
    "    for node_idx in index_anomaly:\n",
    "        f+=1\n",
    "        print(f/len(index_anomaly))\n",
    "        if hops:\n",
    "            if lbl_needed:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"),label=data.y.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "            else:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"))\n",
    "        else:\n",
    "            if lbl_needed:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), label=data.y.to(\"cuda:0\"),edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "            else:\n",
    "                exp=ex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"),y = data.y.to(\"cuda:0\"))\n",
    "        \n",
    "        tempf=np.empty(0)\n",
    "\n",
    "        tempe=np.empty(0)\n",
    "        for i in range(0,k):\n",
    "            if feature:\n",
    "                tempf=np.append(tempf,count(gt[node_idx][0].feature_imp,get_topk_mask(exp.feature_imp,i)))\n",
    "                tempf=np.append(tempf,Charax(data,get_topk_mask(exp.feature_imp,i),node_idx))\n",
    "            if edge:\n",
    "                tempe=np.append(tempe,count(gt[node_idx][0].edge_imp,get_topk_mask(exp.edge_imp,i*2)))\n",
    "                sub = k_hop_subgraph(node_idx, 3,data.edge_index)\n",
    "                tempe=np.append(tempe,Charas(data,get_topk_mask(exp.edge_imp,i*2),sub,node_idx))\n",
    "        if feature:\n",
    "            moyfeature=np.vstack([moyfeature,tempf])\n",
    "        if edge:\n",
    "            moyedge=np.vstack([moyedge,tempe])\n",
    "    return moyfeature.mean(axis=0),moyedge.mean(axis=0)\n",
    "def meancount(gt,s,x,index_anomaly,data,kx,ks):\n",
    "   \n",
    "    moyfeature=np.empty(3)\n",
    "    moyedge=np.empty(3)\n",
    "    for node_idx in index_anomaly:\n",
    "        subset, sub_edge_index, mapping, hard_edge_mask = k_hop_subgraph(node_idx, 3, data.edge_index, relabel_nodes=True)\n",
    "        subs=np.empty(0)\n",
    "        st=s.detach().cpu().numpy()\n",
    "        for i in range(0,len(sub_edge_index[0])):\n",
    "            subs=np.append(subs,st[sub_edge_index[0][i]][sub_edge_index[1][i]]+st[sub_edge_index[1][i]][sub_edge_index[0][i]])\n",
    "        moyfeature=np.vstack([moyfeature,count(gt[node_idx][0].feature_imp,get_topk_mask(x[node_idx],kx))])\n",
    "        moyedge=np.vstack([moyedge,count(gt[node_idx][0].edge_imp,get_topk_mask(torch.from_numpy(subs),ks))])\n",
    "    return moyfeature.mean(axis=0),moyedge.mean(axis=0)\n",
    "def meancountGNNEx(gt,exp,index_anomaly,model,data,k,f):\n",
    "    moyfeature=np.empty(k)\n",
    "    moyedge=np.empty(k)\n",
    "    final=[]\n",
    "    tempx=[]\n",
    "    tempa=[]\n",
    "    for node_idx in index_anomaly:\n",
    "        print(data.x.to(\"cuda:0\"))\n",
    "        print(data.edge_index.to(\"cuda:0\"))\n",
    "        print(node_idx)\n",
    "        print(data.y)\n",
    "        e=exp.forward(model=model,x=data.x.to(\"cuda:0\"),edge_index=data.edge_index.to(\"cuda:0\"),target=data.y[node_idx],index=node_idx)\n",
    "        x_imp=e.node_mask\n",
    "        a_imp=e.edge_mask\n",
    "        subset, sub_edge_index, mapping, hard_edge_mask = k_hop_subgraph(node_idx, 3, data.edge_index, relabel_nodes=True)\n",
    "        for i in range(0,k):\n",
    "            moyfeature[i]=count(gt[node_idx][0].feature_imp,get_topk_mask(x_imp,kx))\n",
    "            moyedge[i]=moyedge,count(gt[node_idx][0].edge_imp,get_topk_mask(a_imp,ks))\n",
    "        tempx=np.vstack([tempx,moyfeature])\n",
    "        tempx=np.vstack([tempa,moyedge])\n",
    "    f.write(f\"GNNEx2;{i};{tempx.mean(axis=0)};{tempa.mean(axis=0)}\\n\")\n",
    "    f.close()\n",
    "    return moyfeature.mean(axis=0),moyedge.mean(axis=0)\n",
    "\n",
    "array=np.column_stack((x.detach().cpu().numpy(),s.detach().cpu().numpy()))\n",
    "#exp=GNNExplainer(modeld)\n",
    "#exp.connect(explainer_config=conf_ex,model_config=conf_m)\n",
    "#sub_ex_exp = sub_ex.get_explanation_node(node_idx = node_idx,x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"))\n",
    "#gnnex_exp = gnnex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"))\n",
    "#ig_exp = igex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "#grad_exp =gradex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "#guid_exp=guidex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "house\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 243/243 [00:01<00:00, 206.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "dummy\n",
      "0.00398406374501992\n",
      "0.00796812749003984\n",
      "0.01195219123505976\n",
      "0.01593625498007968\n",
      "0.0199203187250996\n",
      "0.02390438247011952\n",
      "0.027888446215139442\n",
      "0.03187250996015936\n",
      "0.035856573705179286\n",
      "0.0398406374501992\n",
      "0.043824701195219126\n",
      "0.04780876494023904\n",
      "0.05179282868525897\n",
      "0.055776892430278883\n",
      "0.05976095617529881\n",
      "0.06374501992031872\n",
      "0.06772908366533864\n",
      "0.07171314741035857\n",
      "0.07569721115537849\n",
      "0.0796812749003984\n",
      "0.08366533864541832\n",
      "0.08764940239043825\n",
      "0.09163346613545817\n",
      "0.09561752988047809\n",
      "0.099601593625498\n",
      "0.10358565737051793\n",
      "0.10756972111553785\n",
      "0.11155378486055777\n",
      "0.11553784860557768\n",
      "0.11952191235059761\n",
      "0.12350597609561753\n",
      "0.12749003984063745\n",
      "0.13147410358565736\n",
      "0.13545816733067728\n",
      "0.1394422310756972\n",
      "0.14342629482071714\n",
      "0.14741035856573706\n",
      "0.15139442231075698\n",
      "0.1553784860557769\n",
      "0.1593625498007968\n",
      "0.16334661354581673\n",
      "0.16733067729083664\n",
      "0.17131474103585656\n",
      "0.1752988047808765\n",
      "0.17928286852589642\n",
      "0.18326693227091634\n",
      "0.18725099601593626\n",
      "0.19123505976095617\n",
      "0.1952191235059761\n",
      "0.199203187250996\n",
      "0.20318725099601595\n",
      "0.20717131474103587\n",
      "0.21115537848605578\n",
      "0.2151394422310757\n",
      "0.21912350597609562\n",
      "0.22310756972111553\n",
      "0.22709163346613545\n",
      "0.23107569721115537\n",
      "0.2350597609561753\n",
      "0.23904382470119523\n",
      "0.24302788844621515\n",
      "0.24701195219123506\n",
      "0.250996015936255\n",
      "0.2549800796812749\n",
      "0.2589641434262948\n",
      "0.26294820717131473\n",
      "0.26693227091633465\n",
      "0.27091633466135456\n",
      "0.2749003984063745\n",
      "0.2788844621513944\n",
      "0.28286852589641437\n",
      "0.2868525896414343\n",
      "0.2908366533864542\n",
      "0.2948207171314741\n",
      "0.29880478087649404\n",
      "0.30278884462151395\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from graphxai.explainers import *\n",
    "\n",
    "import time\n",
    "\n",
    "#from torch_geometric.explain import GNNExplainer,DummyExplainer, ExplainerConfig, ModelConfig\n",
    "from graphxai.metrics import graph_exp_faith\n",
    "# Embedding layer name is final GNN embedding layer in the model\n",
    "F=[\"house\",\"circle\",\"flag\"]\n",
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    for form in F:\n",
    "        print(form)\n",
    "        dataset = ShapeGGen(\n",
    "                    base_graph= \"ba\",\n",
    "                    verify=False,\n",
    "                    max_tries_verification=5,\n",
    "                    homophily_coef=1.0,\n",
    "                    seed=0,\n",
    "                    shape_method=form,\n",
    "                    sens_attribution_noise=0.3,\n",
    "                    num_hops=3,\n",
    "                    model_layers=3,\n",
    "                    make_explanations=True,\n",
    "                    variant=1,\n",
    "                    num_subgraphs=300,\n",
    "                    prob_connection=0.003,\n",
    "                    subgraph_size=11,\n",
    "                    # Features=\n",
    "                    class_sep=15.,\n",
    "                    n_features=100,\n",
    "                    n_clusters_per_class=2,\n",
    "                    n_informative=10,\n",
    "                    add_sensitive_feature=False,\n",
    "                    attribute_sensitive_feature=False,\n",
    "        )\n",
    "        data = dataset.get_graph(use_fixed_split=True)\n",
    "        index_anomaly=find_indices(dataset.y,lambda e:e==1)\n",
    "        index_normal=find_indices(dataset.y,lambda e:e==0)\n",
    "        index_anomaly_train=index_anomaly[0:round(len(index_anomaly)/10)]\n",
    "\n",
    "        index_normal_train=index_normal[0:round(len(index_normal)/10)]\n",
    "        modeld = DOMINANT(epoch=200,hid_dim=10,dropout=0.5,alpha=0.5,batch_size=0,gpu=0)\n",
    "        modeld.fit(data)\n",
    "        #D,x,s=modeld.decision_function(data)\n",
    "        pred, D, prob, conf = modeld.predict(data,return_pred=True,return_score=True,return_prob=True,return_conf=True)\n",
    "        pgex = PGExplainer(modeld,in_channels = 10, max_epochs = 10, lr = 0.1)\n",
    "        gnnex = GNNExplainer(modeld)\n",
    "        gradex = GradExplainer(modeld, criterion=criterion)\n",
    "        guidex= GuidedBP(modeld)\n",
    "        randex= RandomExplainer(modeld)\n",
    "        sub_ex = SubgraphX(modeld, reward_method = 'gnn_score', num_hops = 3)\n",
    "        GLEx = GraphLIME(modeld)\n",
    "        LRPEx      = GNN_LRP(modeld)\n",
    "        pgmex=PGMExplainer(modeld, explain_graph=False, p_threshold=0.1)\n",
    "        #exp=GNNExplainer()\n",
    "        #conf_ex=ExplainerConfig(\"phenomenon\",\"attributes\",\"object\")\n",
    "        #conf_m=ModelConfig(\"binary_classification\",\"node\",\"raw\")\n",
    "       # exp.connect(explainer_config=conf_ex,model_config=conf_m)\n",
    "        \n",
    "        # Required to first train PGExplainer on the dataset:\n",
    "        # Feed in entire data, the internal model uses the data's train mask\n",
    "        #pgex.train_explanation_model(data)\n",
    "\n",
    "        # No training with Integrated Gradients, just run the model:\n",
    "        igex = IntegratedGradExplainer(modeld, criterion=criterion)\n",
    "\n",
    "        # Sample a random node for visualization. Also returns a ground truth explanation:\n",
    "        node_idx, gt_exp = dataset.choose_node(split = 'test')\n",
    "        print(node_idx)\n",
    "\n",
    "\n",
    "        # Get explanations from both IG and PGEx:\n",
    "        #guid_exp=GLEx.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "\n",
    "        #guid_exp=LRPEx.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "\n",
    "\n",
    "        #guid_exp=pgmex.get_explanation_node(node_idx = node_idx, x = data.x.to(\"cuda:0\"), edge_index = data.edge_index.to(\"cuda:0\"), y = data.y.to(\"cuda:0\"))\n",
    "\n",
    "        #rand_exp=randex.get_explanation_node(node_idx = node_idx,num_hops=3, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "        #fig, ax = plt.subplots(1,1, figsize = (10, 8))\n",
    "\n",
    "        # Ground-truth explanations always provided as a list. In ShapeGGen, we use the first\n",
    "        #   element since it produces unique explanations. \n",
    "        #gt_exp[0].visualize_node(num_hops = 3, graph_data = data, ax = ax[0])\n",
    "        #pgex_exp.visualize_node(num_hops = 3, graph_data = data, ax = ax[1])\n",
    "        #ig_exp.visualize_node(num_hops = 3, graph_data = data, ax = ax[2])\n",
    "\n",
    "        f=open(f\"C:/Users/ookur/Downloads/result3\"+form+\"v2.csv\",'a')\n",
    "        #meancountGNNEx(dataset.explanations,exp,index_anomaly,modeld.model,data,10,f)\n",
    "\n",
    "        print(\"dummy\")\n",
    "        start = time.time()\n",
    "        temp=meancountexplv2(dataset.explanations,randex,index_anomaly,data,10,edge=True,feature=True,lbl_needed=False)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        f.write(f\"Dummy;{i};\")\n",
    "        \n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        print(\"gnnex\")\n",
    "        print(\"GNNEx\")\n",
    "        start = time.time()\n",
    "        \n",
    "        temp=meancountexplv2(dataset.explanations,gnnex,index_anomaly,data,10,edge=True,feature=True,lbl_needed=False)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        f.write(f\"GNNEx;{i};\")\n",
    "        \n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        print(\"SubGX\")\n",
    "        start = time.time()\n",
    "        \n",
    "        temp=meancountexplv2(dataset.explanations,sub_ex,index_anomaly,data,10,edge=True,feature=False,lbl_needed=False,hops=False)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        f.write(f\"SubGx;{i};\")\n",
    "        \n",
    "        #for i in range(0,30):\n",
    "         #   f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #f.write(f\"SubGX;{i};{meancountexplv2(dataset.explanations,sub_ex,index_anomaly,data,10,edge=True,feature=False,lbl_needed=False,hops=False)}\\n\")\n",
    "        #f.write(f\"GNNEx;{i};{meancountexpl(dataset.explanations,gnnex,index_anomaly,data,10,edge=True,feature=True,lbl_needed=False)}\\n\")\n",
    "\n",
    "    #     print(meancountexpl(dataset.explanations,pgex,index_anomaly,data,i,i,edge=True,feature=False,lbl_needed=False))\n",
    "        f.write(f\"IGEx;{i};\")\n",
    "        start = time.time()\n",
    "        \n",
    "        temp=meancountexplv2(dataset.explanations,igex,index_anomaly,data,10,edge=False,feature=True,lbl_needed=True)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        #for i in range(0,30):\n",
    "         #   f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        f.write(f\"GradEx;{i};\")\n",
    "        start = time.time()\n",
    "        \n",
    "        temp=meancountexplv2(dataset.explanations,gradex,index_anomaly,data,10,edge=False,feature=True,lbl_needed=True)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        #for i in range(0,30):\n",
    "         #   f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        f.write(f\"GuidedBP;{i};\")\n",
    "        start = time.time()\n",
    "        \n",
    "        temp=meancountexplv2(dataset.explanations,guidex,index_anomaly,data,10,edge=False,feature=True,lbl_needed=False,hops=False)\n",
    "        end = time.time()\n",
    "        t=end-start\n",
    "        for i in range(0,50):\n",
    "            f.write(f\"{temp[0][i]};\")\n",
    "        \n",
    "        #for i in range(0,30):\n",
    "         #   f.write(f\"{temp[1][i]};\")\n",
    "        f.write(f\"{t};\\n\")\n",
    "        \n",
    "        #f.write(f\"GradEx;{i};{meancountexpl(dataset.explanations,gradex,index_anomaly,data,10,edge=False,feature=True,lbl_needed=False)}\\n\")\n",
    "        #f.write(f\"GuidedBP;{i};{meancountexpl(dataset.explanations,guidex,index_anomaly,data,10,edge=False,feature=True,lbl_needed=True,hops=False)}\\n\")\n",
    "\n",
    "        # print(meancount(dataset.explanations,ss,sx,index_anomaly,data,0.9,0.9))\n",
    "            #f.write(f\"Ours;{i};{meancount(dataset.explanations,s,x,index_anomaly,data,i,i*2)}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "circle=pd.read_csv(\"C:/Users/ookur/Downloads/result3circlev2.csv\",sep=';',header=None)\n",
    "flag=pd.read_csv(\"C:/Users/ookur/Downloads/result3flagv2.csv\",sep=';',header=None)\n",
    "house=pd.read_csv(\"C:/Users/ookur/Downloads/result3housev2.csv\",sep=';',header=None)\n",
    "print(circle)\n",
    "form=[\"circle\",\"flag\",\"house\"]\n",
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"GNNEx\",\"Ours\"]\n",
    "feature=[\"Dummy\",\"GNNEx\",\"GradEx\"]\n",
    "temp1=[]\n",
    "temp2=[]\n",
    "temp3=[]\n",
    "for k in form:\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10):\n",
    "            temp1.append(temp0[2+i*3].mean())\n",
    "            temp2.append(temp0[2+i*3].std())\n",
    "            temp3.append(temp0[2+i*3].std())\n",
    "        for j in range(0,10):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10), temp1, label=f)\n",
    "        plt.fill_between(range(0,10), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"GEA\")\n",
    "    plt.title(\"GEA@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(k+\"FG\")\n",
    "    plt.show()\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10):\n",
    "            temp1.append(temp0[2+i*3+1][3].mean())\n",
    "            temp2.append(temp0[2+i*3+1][3].std())\n",
    "            temp3.append(temp0[2+i*3+1][3].std())\n",
    "        for j in range(0,10):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10), temp1, label=f)\n",
    "        plt.fill_between(range(0,10), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"FP\")\n",
    "    plt.show()\n",
    "\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10):\n",
    "            temp1.append(temp0[2+i*3+2][4].mean())\n",
    "            temp2.append(temp0[2+i*3+2][4].std())\n",
    "            temp3.append(temp0[2+i*3+2][4].std())\n",
    "        for j in range(0,10):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10), temp1, label=f)\n",
    "        plt.fill_between(range(0,10), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Recall@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"FR\")\n",
    "    plt.show()\n",
    "    edge=[\"Dummy\",\"Ours\",\"SubGX\",\"GNNEx\"]\n",
    "    #edge=[\"SubGX\"]\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][3].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][3].std())\n",
    "                temp3.append(temp0[temp0[1]==i][3].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][5].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][5].std())\n",
    "                temp3.append(temp0[temp0[1]==i][5].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"GEA\")\n",
    "    plt.title(\"GEA@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"EG\")\n",
    "\n",
    "    plt.show()\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][4].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][4].std())\n",
    "                temp3.append(temp0[temp0[1]==i][4].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][6].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][6].std())\n",
    "                temp3.append(temp0[temp0[1]==i][6].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"EP\")\n",
    "    plt.show()\n",
    "\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][5].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][5].std())\n",
    "                temp3.append(temp0[temp0[1]==i][5].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][7].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][7].std())\n",
    "                temp3.append(temp0[temp0[1]==i][7].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Recall@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"ER\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "f=open(f\"C:/Users/ookur/Downloads/result3.csv\",'a')\n",
    "f.write(f\"allo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "CV=pd.read_csv(\"C:/Users/ookur/Downloads/result3circle.csv\",sep=';',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "circle=pd.read_csv(\"C:/Users/ookur/Downloads/result3circle.csv\",sep=';',header=None)\n",
    "\n",
    "flag=pd.read_csv(\"C:/Users/ookur/Downloads/result3flag.csv\",sep=';',header=None)\n",
    "\n",
    "house=pd.read_csv(\"C:/Users/ookur/Downloads/result3house.csv\",sep=';',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "form=[\"circle\",\"flag\",\"house\"]\n",
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"GNNEx\",\"Ours\"]\n",
    "temp1=[]\n",
    "temp2=[]\n",
    "temp3=[]\n",
    "for k in form:\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            temp1.append(temp0[temp0[1]==i][2].mean())\n",
    "            temp2.append(temp0[temp0[1]==i][2].std())\n",
    "            temp3.append(temp0[temp0[1]==i][2].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10,2), temp1, label=f)\n",
    "        plt.fill_between(range(0,10,2), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"GEA\")\n",
    "    plt.title(\"GEA@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(k+\"FG\")\n",
    "    plt.show()\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            temp1.append(temp0[temp0[1]==i][3].mean())\n",
    "            temp2.append(temp0[temp0[1]==i][3].std())\n",
    "            temp3.append(temp0[temp0[1]==i][3].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10,2), temp1, label=f)\n",
    "        plt.fill_between(range(0,10,2), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"FP\")\n",
    "    plt.show()\n",
    "\n",
    "    for f in feature:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            temp1.append(temp0[temp0[1]==i][4].mean())\n",
    "            temp2.append(temp0[temp0[1]==i][4].std())\n",
    "            temp3.append(temp0[temp0[1]==i][4].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,10,2), temp1, label=f)\n",
    "        plt.fill_between(range(0,10,2), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Recall@k on features for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"FR\")\n",
    "    plt.show()\n",
    "    edge=[\"Dummy\",\"Ours\",\"SubGX\",\"GNNEx\"]\n",
    "    #edge=[\"SubGX\"]\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][3].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][3].std())\n",
    "                temp3.append(temp0[temp0[1]==i][3].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][5].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][5].std())\n",
    "                temp3.append(temp0[temp0[1]==i][5].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"GEA\")\n",
    "    plt.title(\"GEA@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"EG\")\n",
    "\n",
    "    plt.show()\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][4].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][4].std())\n",
    "                temp3.append(temp0[temp0[1]==i][4].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][6].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][6].std())\n",
    "                temp3.append(temp0[temp0[1]==i][6].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"EP\")\n",
    "    plt.show()\n",
    "\n",
    "    for f in edge:\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        temp3=[]\n",
    "        temp0=circle[circle[0]==f]\n",
    "        for i in range(0,10,2):\n",
    "            if f==\"SubGX\":\n",
    "                temp1.append(temp0[temp0[1]==i][5].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][5].std())\n",
    "                temp3.append(temp0[temp0[1]==i][5].std())\n",
    "            else:\n",
    "                temp1.append(temp0[temp0[1]==i][7].mean())\n",
    "                temp2.append(temp0[temp0[1]==i][7].std())\n",
    "                temp3.append(temp0[temp0[1]==i][7].std())\n",
    "        for j in range(0,5):\n",
    "            temp2[j]=temp1[j] - temp2[j]\n",
    "            temp3[j]=temp1[j] + temp3[j]\n",
    "        print(temp1)\n",
    "        plt.plot(range(0,20,4), temp1, label=f)\n",
    "        plt.fill_between(range(0,20,4), temp2, temp3, alpha=0.2)\n",
    "        plt.legend()\n",
    "        # Customize the plot (optional)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Recall@k on edges for Shape\"+k)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(k+\"ER\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"Ours\"]\n",
    "\n",
    "for f in feature:\n",
    "    temp=flag[flag[0]==f]\n",
    "    plt.plot(temp[1], temp[2], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"GEA\")\n",
    "plt.title(\"GEA@k on features for ShapeFlag\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FFG\")\n",
    "plt.show()\n",
    "\n",
    "for f in feature:\n",
    "    temp=flag[flag[0]==f]\n",
    "    plt.plot(temp[1], temp[3], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision@k on features for ShapeFlag\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FFP\")\n",
    "plt.show()\n",
    "\n",
    "for f in feature:\n",
    "    temp=flag[flag[0]==f]\n",
    "    plt.plot(temp[1], temp[4], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall@k on features for ShapeFlag\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FFR\")\n",
    "plt.show()\n",
    "edge=[\"Dummy\",\"Ours\",\"SubGX\"]\n",
    "for f in edge:\n",
    "    temp=flag[flag[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[3], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[5], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"GEA\")\n",
    "plt.title(\"GEA@k on edges for ShapeFlag\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FEG\")\n",
    "plt.show()\n",
    "\n",
    "for f in edge:\n",
    "    temp=flag[flag[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[4], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[6], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision@k on edges for ShapeFlag\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FEP\")\n",
    "plt.show()\n",
    "\n",
    "for f in edge:\n",
    "    temp=flag[flag[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[5], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[7], label=f)\n",
    "    \n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall@k on edges for ShapeFlag\")\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"FER\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"Ours\"]\n",
    "\n",
    "for f in feature:\n",
    "    temp=house[house[0]==f]\n",
    "    plt.plot(temp[1], temp[2], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"GEA\")\n",
    "plt.title(\"GEA@k on features for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HFG\")\n",
    "plt.show()\n",
    "\n",
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"Ours\"]\n",
    "\n",
    "for f in feature:\n",
    "    temp=house[house[0]==f]\n",
    "    plt.plot(temp[1], temp[3], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision@k on features for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HFP\")\n",
    "plt.show()\n",
    "feature=[\"Dummy\",\"IGEx\",\"GradEx\",\"GuidedBP\",\"Ours\"]\n",
    "\n",
    "for f in feature:\n",
    "    temp=house[house[0]==f]\n",
    "    plt.plot(temp[1], temp[4], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall@k on features for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HFR\")\n",
    "plt.show()\n",
    "edge=[\"Dummy\",\"Ours\",\"SubGX\"]\n",
    "#edge=[\"SubGX\"]\n",
    "for f in edge:\n",
    "    temp=house[house[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[3], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[5], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"GEA\")\n",
    "plt.title(\"GEA@k on edges for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HEG\")\n",
    "plt.show()\n",
    "\n",
    "for f in edge:\n",
    "    temp=house[house[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[4], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[6], label=f)\n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision@k on edges for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HEP\")\n",
    "plt.show()\n",
    "\n",
    "for f in edge:\n",
    "    temp=house[house[0]==f]\n",
    "    if f==\"SubGX\":\n",
    "        plt.plot(temp[1]*2, temp[5], label=f)\n",
    "    else:\n",
    "        plt.plot(temp[1]*2, temp[7], label=f)\n",
    "    \n",
    "    plt.legend()\n",
    "    # Customize the plot (optional)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall@k on edges for ShapeHouse\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"HER\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
